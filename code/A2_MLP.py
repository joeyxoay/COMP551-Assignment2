# -*- coding: utf-8 -*-
"""COMP551A2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1feq2a2kqjT5ConudTilFNM_MN5UduPt0
"""

import torch
from torchvision import datasets, transforms
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import math
from typing import List
from tqdm import tqdm
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder

# Define a data transform to convert the data to PyTorch tensors
transform = transforms.ToTensor()

# ============ FashionMNIST ============
# Load the training and testing datasets
train_dataset_fashion = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)
test_dataset_fashion = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)
labels_fashion = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
labels_CIFAR10 = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']

# Create data loaders to iterate through the datasets
train_loader_fashion = torch.utils.data.DataLoader(train_dataset_fashion, batch_size=len(train_dataset_fashion), shuffle=True)
test_loader_fashion = torch.utils.data.DataLoader(test_dataset_fashion, batch_size=len(test_dataset_fashion), shuffle=False)

# Access the data and labels
X_train_fashion_raw, y_train_fashion_beforeSplit = next(iter(train_loader_fashion))
X_test_fashion_raw, y_test_fashion = next(iter(test_loader_fashion))

# ============ CIFAR10 ============
# Load the training and testing datasets
train_dataset_CIFAR10 = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
test_dataset_CIFAR10 = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)

# Create data loaders to iterate through the datasets
train_loader_CIFAR10 = torch.utils.data.DataLoader(train_dataset_CIFAR10, batch_size=len(train_dataset_CIFAR10), shuffle=True)
test_loader_CIFAR10 = torch.utils.data.DataLoader(test_dataset_CIFAR10, batch_size=len(test_dataset_CIFAR10), shuffle=False)

# Access the data and labels
X_train_CIFAR10_raw, y_train_CIFAR10_beforeSplit = next(iter(train_loader_CIFAR10))
X_test_CIFAR10_raw, y_test_CIFAR10 = next(iter(test_loader_CIFAR10))

# FASHION
plt.figure(figsize=(8, 8))

rows = 6
columns = 6
for i in range(rows*columns):
  plt.subplot(rows, columns, i+1)
  plt.imshow(X_train_fashion_raw[i].reshape(28, 28))
  plt.xlabel(labels_fashion[y_train_fashion_beforeSplit[i]])
  plt.yticks([])
  plt.xticks([])

plt.show()

# CIFAR 10
plt.figure(figsize=(8, 8))

rows = 6
columns = 6

for i in range(rows * columns):
    plt.subplot(rows, columns, i + 1)
    plt.imshow(X_train_CIFAR10_raw[i].permute(1, 2, 0))
    plt.xlabel(labels_CIFAR10[y_train_CIFAR10_beforeSplit[i]])
    plt.yticks([])
    plt.xticks([])

plt.show()

"""# Task 2: Implement a Multilayer Perceptron"""

def preprocess_data(X_train, X_test):
    # Convert to float32 for numerical stability
    #Dont remove, else it UFuncTypeError: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind'
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')

    # Mean subtraction
    mean_image = np.mean(X_train, axis=0)
    X_train -= mean_image
    X_test -= mean_image

    # Normalization
    std_dev = np.std(X_train, axis=0)
    std_dev[std_dev == 0] = 1  # avoid division by zero
    X_train /= std_dev
    X_test /= std_dev

    return X_train, X_test

def load_dataset(dataset_name, normalize=True):
    if dataset_name == "fashion_mnist":
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
    elif dataset_name == "cifar10":
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
    else:
        raise ValueError("Unknown dataset: " + dataset_name)

    # Preprocess the data
    if normalize:
      X_train, X_test = preprocess_data(X_train, X_test)
    X_train = X_train.reshape(X_train.shape[0], -1)  # Flatten
    X_test = X_test.reshape(X_test.shape[0], -1)     # Flatten

    # Reshape labels to be 2D arrays
    y_train = y_train.reshape(-1, 1)
    y_test = y_test.reshape(-1, 1)

    # One-hot encode the labels
    one_hot_encoder = OneHotEncoder(sparse=False)
    y_train = one_hot_encoder.fit_transform(y_train)
    y_test = one_hot_encoder.transform(y_test)

    return X_train, X_test, y_train, y_test

"""# Task 2

"""

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

def hyperbolic_tangent(x):
    return np.tanh(x)

def hyperbolic_tangent_derivative(x):
    return 1-np.tanh(x)**2

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def leaky_relu_derivative(x, alpha=0.01):
    return np.where(x > 0, 1.0, alpha)

def softmax(x):
    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return e_x / np.sum(e_x, axis=-1, keepdims=True)

def evaluate_acc(y_true, y_pred):
    return np.mean(y_true == y_pred)

class MLP:
    def __init__(self, input_size, hidden_layers, output_size, activation_func=relu, activation_derivative=relu_derivative, initializer='xavier', reg_type=None, reg_coeff=0.0):
        self.layers = [input_size] + hidden_layers + [output_size]
        self.params = {}
        self.activation_func = activation_func
        self.activation_derivative = activation_derivative
        self.init_weights(initializer)
        self.reg_type = reg_type
        self.reg_coeff = reg_coeff

    def init_weights(self, initializer):
        for i in range(1, len(self.layers)):
          if initializer == 'zeros':
              self.params['W' + str(i)] = np.zeros((self.layers[i - 1], self.layers[i]))
          elif initializer == 'uniform':
              self.params['W' + str(i)] = np.random.uniform(-1, 1, (self.layers[i - 1], self.layers[i]))
          elif initializer == 'gaussian':
              self.params['W' + str(i)] = np.random.randn(self.layers[i - 1], self.layers[i])
          elif initializer == 'xavier':
              stddev = np.sqrt(1 / self.layers[i - 1])
              self.params['W' + str(i)] = np.random.randn(self.layers[i - 1], self.layers[i]) * stddev
          elif initializer == 'kaiming':
              stddev = np.sqrt(2 / self.layers[i - 1])
              self.params['W' + str(i)] = np.random.randn(self.layers[i - 1], self.layers[i]) * stddev
          else:
              raise ValueError("Unknown initializer: " + initializer)

          self.params['b' + str(i)] = np.zeros((1, self.layers[i]))

    def forward(self, X):
        cache = {'A0': X}
        for i in range(1, len(self.layers)):
            #Z means weighted sum to each layer
            #A means the activation output for each layer
            cache['Z' + str(i)] = np.dot(cache['A' + str(i - 1)], self.params['W' + str(i)]) + self.params['b' + str(i)]
            if i != len(self.layers) - 1:
              cache['A' + str(i)] = self.activation_func(cache['Z' + str(i)])
            else:
              cache['A' + str(i)] = softmax(cache['Z' + str(i)])
        return cache

    def backward(self, cache, y):
        m = y.shape[0]
        grads = {}
        dZ = self.cross_entropy_derivative(y, cache['A' + str(len(self.layers) - 1)])
        for i in range(len(self.layers) - 1, 0, -1):
            grads['dW' + str(i)] = np.dot(cache['A' + str(i - 1)].T, dZ) / m
            if self.reg_type == 'l1':
                grads['dW' + str(i)] += self.grad_l1_regularization(self.params['W' + str(i)]) / m
            elif self.reg_type == 'l2':
                grads['dW' + str(i)] += self.grad_l2_regularization(self.params['W' + str(i)]) / m

            grads['db' + str(i)] = np.sum(dZ, axis=0, keepdims=True) / m
            if i > 1:
                dA = np.dot(dZ, self.params['W' + str(i)].T)
                dZ = dA * self.activation_derivative(cache['Z' + str(i - 1)])
        return grads

    def update_params(self, grads, learning_rate):
        for i in range(1, len(self.layers)):
            self.params['W' + str(i)] -= learning_rate * grads['dW' + str(i)]
            self.params['b' + str(i)] -= learning_rate * grads['db' + str(i)]

    def fit(self, X, y, learning_rate=0.01, epochs=100, batch_size=32):
        accuracies = []
        for epoch in range(epochs):
            indices = np.arange(X.shape[0])
            np.random.shuffle(indices)
            X = X[indices]
            y = y[indices]
            for i in range(0, X.shape[0], batch_size):
                X_batch = X[i:i + batch_size]
                y_batch = y[i:i + batch_size]
                cache = self.forward(X_batch)
                grads = self.backward(cache, y_batch)
                self.update_params(grads, learning_rate)
            if (epoch + 1) % 10 == 0:
                predictions = self.predict(X)
                accuracy = evaluate_acc(np.argmax(y, axis=1), np.argmax(predictions, axis=1))
                loss = self.cross_entropy(y, predictions)
                accuracies.append(accuracy)
                print("Epoch [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%".format(epoch + 1, epochs, loss, accuracy * 100))
        return accuracies

    def predict(self, X):
        return self.forward(X)['A' + str(len(self.layers) - 1)]

    def cross_entropy(self, y_true, y_pred):
        loss = -np.sum(y_true * np.log(y_pred + 1e-10)) / y_true.shape[0]
        if self.reg_type == 'l1':
            for i in range(1, len(self.layers)):
                loss += self.l1_regularization(self.params['W' + str(i)]) / y_true.shape[0]
        elif self.reg_type == 'l2':
            for i in range(1, len(self.layers)):
                loss += self.l2_regularization(self.params['W' + str(i)]) / y_true.shape[0]

        return loss

    def cross_entropy_derivative(self, y_true, y_pred):
        return y_pred - y_true

    def l1_regularization(self, W):
        return self.reg_coeff * np.sum(np.abs(W))

    def l2_regularization(self, W):
        return 0.5 * self.reg_coeff * np.sum(np.square(W))

    def grad_l1_regularization(self, W):
        return self.reg_coeff * np.sign(W)

    def grad_l2_regularization(self, W):
        return self.reg_coeff * W

"""# Task 3"""

dataset_name = "fashion_mnist"  # or "cifar10"
X_train, X_test, y_train, y_test = load_dataset(dataset_name)

# Initialize the MLP
input_size = X_train.shape[1]
hidden_layers = [128]
output_size = y_train.shape[1]

models = {
    "Zeros": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, initializer='zeros'),
    "Uniform": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, initializer='uniform'),
    "Gaussian": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, initializer='gaussian'),
    "Xavier": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, initializer='xavier'),
    "Kaiming": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, initializer='kaiming')
}

learning_rate = 0.01
epochs = 100
batch_size = 32

results = {}
epochs_x = [10,20,30,40,50,60,70,80,90,100]

for method, model in models.items():
    print(f"Training model with weight initialization: {method}")

    # Train the model
    train_accuracies = model.fit(X_train, y_train, learning_rate=learning_rate, epochs=epochs, batch_size=batch_size)

    predictions = model.predict(X_test)
    accuracy = evaluate_acc(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))

    results[method] = {
        "model": model,
        "testAccuracy": accuracy,
        "trainAccuracies": train_accuracies
    }
    print(f"Test accuracy for {method}: {accuracy:.2f}")

i=0.00
for method, result in results.items():
    i -= 0.01
    print(result)
    train_accuracies = result["trainAccuracies"]
    plt.plot(epochs_x, train_accuracies, label="{}. Test accuracy: {:.2%}".format(method, result["testAccuracy"]))
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Different weight initialized training accuracy over epochs')
plt.legend()
plt.show()

dataset_name = "fashion_mnist"  # or "cifar10"
X_train, X_test, y_train, y_test = load_dataset(dataset_name)

# Initialize the MLP
input_size = X_train.shape[1]
hidden_layer_units = 128
output_size = y_train.shape[1]

models = {
    "zero": MLP(input_size=input_size, hidden_layers=[], output_size=output_size, activation_func=relu),
    "one": MLP(input_size=input_size, hidden_layers=[hidden_layer_units], output_size=output_size, activation_func=relu),
    "two": MLP(input_size=input_size, hidden_layers=[hidden_layer_units, hidden_layer_units], output_size=output_size, activation_func=relu)
}

learning_rate = 0.01
epochs = 100
batch_size = 32

results = {}
epochs_x = [10,20,30,40,50,60,70,80,90,100]

for method, model in models.items():
    print(f"Training model with {method} hidden layers")

    # Train the model
    train_accuracies = model.fit(X_train, y_train, learning_rate=learning_rate, epochs=epochs, batch_size=batch_size)

    predictions = model.predict(X_test)
    accuracy = evaluate_acc(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))

    results[method] = {
        "model": model,
        "testAccuracy": accuracy,
        "trainAccuracies": train_accuracies
    }
    print(f"Test accuracy for {method}: {accuracy:.2f}")

for method, result in results.items():
    print(result)
    train_accuracies = result["trainAccuracies"]
    plt.plot(epochs_x, train_accuracies, label="{} hidden layers. Test accuracy: {:.2%}".format(method, result["testAccuracy"]))
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("Different amount of hidden layers's training accuracy over epochs")
plt.legend()
plt.show()

dataset_name = "fashion_mnist"  # or "cifar10"
X_train, X_test, y_train, y_test = load_dataset(dataset_name)

# Initialize the MLP
input_size = X_train.shape[1]
hidden_layers = [128, 128]
output_size = y_train.shape[1]

models = {
    "hyperbolic tangent": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, activation_func=hyperbolic_tangent, activation_derivative=hyperbolic_tangent_derivative),
    "leaky relu": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, activation_func=leaky_relu, activation_derivative=leaky_relu_derivative)
}

learning_rate = 0.01
epochs = 100
batch_size = 32

results = {}
epochs_x = [10,20,30,40,50,60,70,80,90,100]

for method, model in models.items():
    print(f"Training model with {method} activation function")

    # Train the model
    train_accuracies = model.fit(X_train, y_train, learning_rate=learning_rate, epochs=epochs, batch_size=batch_size)

    predictions = model.predict(X_test)
    accuracy = evaluate_acc(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))

    results[method] = {
        "model": model,
        "testAccuracy": accuracy,
        "trainAccuracies": train_accuracies
    }
    print(f"Test accuracy for {method}: {accuracy:.2f}")

for method, result in results.items():
    print(result)
    train_accuracies = result["trainAccuracies"]
    plt.plot(epochs_x, train_accuracies, label="{}. Test accuracy: {:.2%}".format(method, result["testAccuracy"]))
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("Different activation function's training accuracy over epochs")
plt.legend()
plt.show()

dataset_name = "fashion_mnist"  # or "cifar10"
X_train, X_test, y_train, y_test = load_dataset(dataset_name)
X_train_raw, X_test_raw, y_train_raw, y_test_raw = load_dataset(dataset_name, normalize=False)

# Initialize the MLP
input_size = X_train.shape[1]
hidden_layers = [128, 128]
output_size = y_train.shape[1]

models = {
    "no regularization": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size),
    "L1": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, reg_type='l1', reg_coeff=0.01),
    "L2": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size, reg_type='l2', reg_coeff=0.01)
}

learning_rate = 0.01
epochs = 100
batch_size = 32

results = {}
epochs_x = [10,20,30,40,50,60,70,80,90,100]

for method, model in models.items():
    print(f"Training model with {method}")

    # Train the model
    train_accuracies = model.fit(X_train, y_train, learning_rate=learning_rate, epochs=epochs, batch_size=batch_size)

    predictions = model.predict(X_test)
    accuracy = evaluate_acc(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))

    results[method] = {
        "model": model,
        "testAccuracy": accuracy,
        "trainAccuracies": train_accuracies
    }
    print(f"Test accuracy for {method}: {accuracy:.2f}")

for method, result in results.items():
    print(result)
    train_accuracies = result["trainAccuracies"]
    plt.plot(epochs_x, train_accuracies, label="{}. Test accuracy: {:.2%}".format(method, result["testAccuracy"]))
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("No regularization vs L1 vs L2's training accuracy over epochs")
plt.legend()
plt.show()

dataset_name = "fashion_mnist"  # or "cifar10"
X_train, X_test, y_train, y_test = load_dataset(dataset_name)
X_train_raw, X_test_raw, y_train_raw, y_test_raw = load_dataset(dataset_name, normalize=False)

# Initialize the MLP
input_size = X_train.shape[1]
hidden_layers = [128, 128]
output_size = y_train.shape[1]

models = {
    "normalized": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size),
    "unnormalized": MLP(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size)
}

learning_rate = 0.01
epochs = 100
batch_size = 32

results = {}
epochs_x = [10,20,30,40,50,60,70,80,90,100]

for method, model in models.items():
    print(f"Training model with {method} activation function")

    # Train the model
    if (method == "unnormalized"):
      X_train = X_train_raw
      X_test = X_test_raw
      y_train = y_train_raw
      y_test = y_test_raw
    train_accuracies = model.fit(X_train, y_train, learning_rate=learning_rate, epochs=epochs, batch_size=batch_size)

    predictions = model.predict(X_test)
    accuracy = evaluate_acc(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))

    results[method] = {
        "model": model,
        "testAccuracy": accuracy,
        "trainAccuracies": train_accuracies
    }
    print(f"Test accuracy for {method}: {accuracy:.2f}")

for method, result in results.items():
    print(result)
    train_accuracies = result["trainAccuracies"]
    plt.plot(epochs_x, train_accuracies, label="{}. Test accuracy: {:.2%}".format(method, result["testAccuracy"]))
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("Normalized image vs unnormalized image's training accuracy over epochs")
plt.legend()
plt.show()

"""## 3.7 Train MLP on the CIFAR-10 Dataset"""

dataset_name = "cifar10"
X_train, X_test, y_train, y_test = load_dataset(dataset_name)

# Initialize the MLP
input_size = X_train.shape[1]
hidden_layer_units = 128
output_size = y_train.shape[1]

models = {
    "two": MLP(input_size=input_size, hidden_layers=[hidden_layer_units, hidden_layer_units], output_size=output_size, activation_func=relu, initializer='kaiming')
}

learning_rate = 0.01
epochs = 100
batch_size = 32

results = {}
epochs_x = [10,20,30,40,50,60,70,80,90,100]

for method, model in models.items():
    print(f"Training model with {method} hidden layers")

    # Train the model
    train_accuracies = model.fit(X_train, y_train, learning_rate=learning_rate, epochs=epochs, batch_size=batch_size)

    predictions = model.predict(X_test)
    accuracy = evaluate_acc(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))

    results[method] = {
        "model": model,
        "testAccuracy": accuracy,
        "trainAccuracies": train_accuracies
    }
    print(f"Test accuracy for {method}: {accuracy:.2f}")

for method, result in results.items():
    print(result)
    train_accuracies = result["trainAccuracies"]
    plt.plot(epochs_x, train_accuracies, label="{} hidden layers. Test accuracy: {:.2%}".format(method, result["testAccuracy"]))
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title("Different amount of hidden layers's training accuracy over epochs")
plt.legend()
plt.show()